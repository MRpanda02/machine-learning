# 1. 机器学习概述

机器学习是人工智能的一个子集,机器被训练从过去的经验中学习.过去的经验是通过手机的数据发展起来的,然后通过朴素贝叶斯,支持向量机(SVM)等算法得出最终结果.

### X = 统计:

统计学是数学的一个分支,它利用整个总体或从总体中抽取的样本来进行分析和推论.是通的一些统计技术是回归,方差,标准偏差,条件概率等.

> 要了解此主题,https://www.analyticsvidhya.com/blog/2014/07/statistics/ (使用统计:如何理解人口分布?)

理解人口分布的概念:

了解参数和总体的各种分布.

理解分布最畅通的方法大致以是使用**直方图**绘制它们,在连续变量的情况下,直方图表示概率分布函数.

有三种不同的度量,需要了解分布:

- 集中趋势测度
- 分散的测量
- 描述曲线形状的度量

#### 集中趋势测度:

中心趋势的度量是帮助通过单一指标描述总体的度量.

以下是集中趋势的度量:

- 平均值
- 中位数 -- 将人口一分为二的值
- 众数 -- 群体中出现频率最高的值

其中平均值受异常值(异常高或异常低)影响最大,其次是中值和众数

#### 离散度量:

离散度两解释了人口如何围绕集中趋势度量分布

- 范围 - 总体中最大值和最小值的差异
- 四分位数 -- 将人口分为4个相等的子集(通常称为第一四分位数,第二四分位数和第三四分位数)
- 四分位数间距 -- 第三四分位数(Q3)和第一四分位数(Q1)的差值.根据四分位数的定义,50%的人口位于四分位数范围内
- 方差 -- 与均值的平方差的平均值
- 标准差 -- 是方差的平方根

![standard_deviation](E:\moon\大二\计算机视觉+深度学习\first_month\standard_deviation.png)

具有相同均值,中位数和众数的2个群体的分布差



#### 描述分布形状的度量:

- 偏度 - 偏度是不对称性的度量,负偏写曲线的左尾很长,反之亦然
- 峰度 - 峰度是"峰值"的量度,具有较高峰值的分布具有正峰度,反之亦然

![](E:\moon\大二\计算机视觉+深度学习\first_month\skewness-and-kurtosis.gif)

> 箱线图是了解分布的最简单,最直观的方法之一,它们在单个图上显示平均值,中位数,四分位数和异常值

![](E:\moon\大二\计算机视觉+深度学习\first_month\box-plot.gif)

- 针对不同的人口类别/细分是同彼此相邻的箱线图,以了解人口中的重叠/差异,以下是此类比较的实力(带有说明性数据):

![](E:\moon\大二\计算机视觉+深度学习\first_month\box_plot_comparison.png)

#### 统计在机器学习中的使用

我们可以使用一种称为朴素贝叶斯的机器学习算法,改算法将检查过去垃圾邮件的频率.以将新电子邮件识别为垃圾邮件.朴素贝叶斯使用统计技术贝叶斯定理(通常称为条件概率),**因此,我们可以说机器学习算法是统计概念来执行机器学习**

#### X = 深度学习:

深度学习与机器学习算法(人工神经网络,KNN)相关联,该算法是同人脑的概念来促进任意函数的建模,ANN需要大象数据,当同时对多个输出进行建模时,该算法非常灵活.

#### X = 数据挖掘:

机器学习和数据挖掘易混淆.

数据挖掘即搜索特定信息,而机器学习只专注于执行给定的任务

![](E:\moon\大二\计算机视觉+深度学习\first_month\teach-ML.png)

机器学习的过程可以分为三个部分

- 输入数据
- 抽象(abstracting)数据
  - 通过选用算法将数据抽象表示,这里进行初期的学习
- 概括(generalization)

这三个步骤可确保机器的整体学习以同等重要的方式执行给定的任务。机器学习的成功取决于两个因素：

1. 抽象数据的泛化程度如何
2. 机器如何将其学习用于预测未来行动的实际用途

#### 机器学习中使用的步骤是什么？

执行机器学习任务有5个基本步骤：

1. **收集数据**: 无论是来自excel,access,文本文件等的原始数据，这一步(收集过去的数据)构成了未来学习的基础，相关数据的种类，密度和数量越好，机器的学习前景就越好
2. **准备数据**:任何分析过程都依赖于所用数据的质量，人们需要花时间确定数据的质量，然后采取措施解决诸如丢失数据和处理异常值等问题。探索性分析可能是详细研究数据细微差别的一种方法，从而增加数据的营养成分
3. **训练模型**:这一步涉及选择合适的算法和模型形式的数据表示。清洗后的数据分为两部分--训练和测试(比例取决于先决条件)；
   - 第一部分(训练数据)：用于开发模型
   - 第二部分(测试数据)：用作参考
4. **评估模型**:为了测试准确性，使用数据的第二部分(保持/测试数据)。此步骤根据结果确定算法选择的精度。检查模型准确性的更好测试时查看其在模型构建期间根本未使用的数据上的性能。
5. **提高性能**:此步骤可能涉及完全选择不同的模型或引入更多变量以提高效率。这就是为什么需要在数据收集和准备上花费大量时间的原因

#### 机器学习算法有哪些类型？

![](E:\moon\大二\计算机视觉+深度学习\first_month\machine-learning-types-850x540.png)

##### 监督学习/预测模型：

预测模型顾名思义就是根据历史数据预测未来的结果。预测模型通常从一开始就给出明确的指示，比如需要学习什么以及如何学习，这类学习算法被称为**监督学习**。

例如：预测发生地震，龙卷风等危险的可能性，以确定总保险价值。

> 算法实例：最近邻(KNN)，朴素贝叶斯，决策树，回归等。

##### 无监督学习/描述性模型：

它用于训练没有设置目标并且没有单个特征比另一个更重要的描述性模型。无监督学习的情况可以是：售商希望找出产品组合是什么时，客户往往会更频繁地购买。此外，在制药行业，无监督学习可用于预测哪些疾病可能与糖尿病一起发生。

> 算法实例: K-means Clustering Algorithm

##### 强化学习(RL):

这时机器学习的一个例子，其中机器被训练以根据业务需求做出特定决策，唯一的座右铭是最大限度地提高效率(性能)。强化学习的思想是：机器/软件代理根据它所接触的环境不断训练自己，并应用它丰富的知识来解决业务问题。这种持续的学习过程可确保减少人类专业知识的参与，从而节省大量时间。

> 算法实例：马尔可夫决策过程(Markov Decision Process)

监督学习和强化学习之间存在细微差别。强化学习本质上涉及通过与环境交互来学习。RL代理从其过去的经验中学习，从其持续的试错学习过程中学习，而不是从外部监督者提供实例的监督学习中学习。

理解差异的一个很好的例子是自动驾驶汽车。自动驾驶汽车使用强化学习不断做出决策 -- 走哪条路？以什么速度行驶？是在与环境互动后决定的一些问题。监督学习的一个简单表现是预测出租车从一个地方到另一个地方的票价。

#### 机器学习有哪些应用?

- **银行和金融服务**：ML可用于预测可能拖欠贷款或信用卡账单的客户。这一点至关重要，因为机器学习将帮助银行识别可以获得贷款和信用卡的客户。
- **医疗保健**：它用于根据患者的症状诊断致命疾病(例如癌症)，并将其与类似患者的过去数据匹配。

# 2. 数据探索综合指南

## 0. 概述：

- 数据探索(EDA)的完整过程
- 涵盖了几个数据探索方面，包括缺失值插补(missing value imputation)，异常值去除(outlier removal)和特征工程艺术(the art of feature engineering)

> 数据探索没有捷径可走。一段时间后，会意识到自己正在努力提高模型的准确性。

## 1. 数据探索和准备步骤

***输入的质量决定了输出的质量***

数据探索，清理和准备可能会占用整个项目时间的70%

以下是理解，清洁和准备数据以构建预测模型所涉及的步骤：

1. 变量标识
2. 单变量分析
3. Bi-variate Analysis
4. 缺失值处理
5. 离群值处理
6. 变量变换
7. 变量创建

最后，我们需要多次迭代第4步到第7步，才能提出我们的精炼模型。

### 1. 变量标识

确定预测变量(输入)和目标(输出)变量、接下来确定变量的数据类型和类别。

### 2. 单变量分析

执行单变量分析的方法取决于变量类型是分类的还是连续的。

**连续变量**：在连续变量的情况下，我们需要了解变量的集中趋势和分布。

![](E:\moon\大二\计算机视觉+深度学习\first_month\Data_exploration_31-850x152.webp)

单变量分析也用于突出缺失值和异常值。

分类变量：对于分类变量，我们将使用频率表来了解每个类别的分布。我们也可以解读为每个类别下值的百分比。它可以使用两个指标来衡量，Count和Count%针对每个类别。条形图可用作可视化。

### Bi-variate Analysis(双变量分析)

双变量分析找出两个变量之间的关系。我们在预定义的显著性水平上寻找变量之间的关联和分类。我们可以对分类变量和连续变量的任意组合进行双变量分析。组合可以是：分类和分类，分类和连续以及连续和连续。在分析过程中使用不同的方法来处理这些组合。

**连续和连续**：在两个连续变量之间进行双变量分析时，我们应该查看散点图。散点图的模式表示变量之间的关系。该关系可以是线性的或非线性的。

![](E:\moon\大二\计算机视觉+深度学习\first_month\Data_exploration_4.png)

散点图显示了两个变量之间的关系，但不表示它们之间的关系强度。为了找到关系强度，我们使用相关性。相关性在-1和+1之间变化。

- -1：完美的负线性相关

- +1：完美的正线性相关

- 0：无相关性

  ​									**相关性 = 协方差(X,Y) / SQRT( Var(X)\* Var(Y))**

**分类和分类**：要找到两个分类变量之间的关系，我们可以使用以下方法：

- 双向表：可以通过创建count和count%的双向表来开始分析关系。行代表一份变量的类别，列代表另一个变量的类别。我们在行和列类别的每个组合中显示可用观察的计数或计数百分比。

![](E:\moon\大二\计算机视觉+深度学习\first_month\Data_exploration_6.gif)

- 卡方检验(Chi-Square Test)：该检验用于推导出变量之间关系的统计显著性。此外，它还测试样本中的数据是否足够强大，可以将这种关系推广到更大的群体。卡方基于双向表中一个或多个类别的预期频率和观察频率之间的差异。它返回具有自由度的计算卡方分布的概率

概率为0：表示两个分类变量是相关的

概率为1：表示两个变量是独立的

概率小于0.05：表示变量之间的关系在95%的置信度下显著，两个分类变量的独立性检验的卡方检验统计量通过以下方式找到：

![](E:\moon\大二\计算机视觉+深度学习\first_month\Data_exploration_7.webp)

其中O表示观察到的频率，E是原假设下的预期频率。

**分类和连续**:在探索分类变量和连续变量之间的关系时，我们可以为分类变量的每个级别绘制箱线图。要查看统计显着性，我们可以执行 Z 检验、T 检验或方差分析。

- Z检验/T检验：任一检验评估两组的平均值是否在统计上彼此不同。

![](E:\moon\大二\计算机视觉+深度学习\first_month\ttest.webp)

如果 Z 的概率很小，则两个平均值的差异更显着。T 检验与 Z 检验非常相似，但在两个类别的观察数小于 30 时使用。

- 方差分析：它评估两组以上的平均值是否具有统计学差异

## 2. 缺失值处理

> Q：为什么需要进行缺失值处理？
>
> A：训练集中的缺失数据会降低模型的功效/拟合，或者可能导致模型有偏差，因为我们没有正确分析行为和与其他变量的关系。它可能导致错误的预测或分类。
>
> Q：为什么我的数据有缺失值？
>
> A：可能发生在两个阶段：
>
> 1. **数据提取**：提取过程可能存在问题。在这种情况下，我们应该与数据监护人一起仔细检查正确的数据。一些散列程序也可用于确保数据提取正确。数据提取阶段的错误通常很容易发现，也很容易纠正。
> 2. 数据收集：这些错误发生在数据收集时，更难纠正。它们可以分为四种类型：
>    - **完全随机丢失：** 这是所有观测值丢失变量的概率相同的情况。例如：数据收集过程的受访者决定在掷一枚公平的硬币后宣布他们的收入。如果出现人头，受访者会申报他/她的收入，反之亦然。这里每个观察值都有相等的缺失值机会。
>    - **随机缺失：**这是随机缺失变量并且缺失率因其他输入变量的不同值/水平而变化的情况。例如：我们正在收集年龄数据，与男性相比，女性的缺失值更高。
>    - **缺失取决于未观察到的预测变量：** 这是缺失值不是随机的并且与未观察到的输入变量相关的情况。例如：在医学研究中，如果特定诊断引起不适，则退出研究的可能性更高。这个缺失值不是随机的，除非我们将“不适”作为所有患者的输入变量。
>    - **缺失取决于缺失值本身：**这是缺失值概率与缺失值本身直接相关的情况。例如： 收入较高或较低的人很可能对他们的收入没有反应。

### 处理缺失值的方法有哪些？

1. 删除：分为两种：List Wise Deletion 和 Pair Wise Deletion

   - 在列表明智的删除中，我们删除缺少任何变量的观察。简单性是这种方法的主要优点之一，但这种方法降低了模型的功效，因为它减少了样本量。

   - 在成对删除中，我们对存在感兴趣变量的所有情况进行分析。这种方法的优点是，它保留了尽可能多的案例可用于分析。这种方法的缺点之一是它对不同的变量使用不同的样本量。

     ## **概述**

     - 数据探索 (EDA) 的完整教程
     - 我们涵盖了几个数据探索方面，包括缺失值插补、异常值去除和特征工程艺术

      

     ## 介绍

     数据探索没有捷径可走。如果你有这样的心态，机器学习可以让你远离每一次数据风暴，相信我，它不会。一段时间后，您会意识到自己正在努力提高模型的准确性。在这种情况下，数据探索技术将派上用场。

     I can confidently say this, because I’ve been through such situations, a lot.

     I have been a Business Analytics professional for close to three years now. In my initial days, one of my mentor suggested me to spend significant time on exploration and analyzing data. Following his advice has served me well.

     I’ve created this tutorial to help you understand the underlying techniques of data exploration. As always, I’ve tried my best to explain these concepts in the simplest manner. For better understanding, I’ve taken up few examples to demonstrate the complicated concepts.

      

     [![分析中数据探索的完整教程](https://www.analyticsvidhya.com/wp-content/uploads/2016/01/de.jpg)](https://www.analyticsvidhya.com/wp-content/uploads/2016/01/de.jpg)

      

     ## Table of Contents

     1. **[Steps of Data Exploration and Preparation](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#one)**
     2. [Missing Value Treatment](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#two)
        - Why missing value treatment is required ?
        - Why data has missing values?
        - Which are the methods to treat missing value ?
     3. [Techniques of Outlier Detection and Treatment](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#three)
        - What is an outlier?
        - What are the types of outliers ?
        - What are the causes of outliers ?
        - What is the impact of outliers on dataset ?
        - How to detect outlier ?
        - How to remove outlier ?
     4. [The Art of Feature Engineering](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#four)
        - What is Feature Engineering ?
        - What is the process of Feature Engineering ?
        - What is Variable Transformation ?
        - When should we use variable transformation ?
        - What are the common methods of variable transformation ?
        - What is feature variable creation and its benefits ?

      

     

     ### Let’s get started.

      

     ## 1. Steps of Data Exploration and Preparation

     Remember the quality of your inputs decide the quality of your output. So, once you have got your business hypothesis ready, it makes sense to spend lot of time and efforts here. With my personal estimate, data exploration, cleaning and preparation can take up to 70% of your total project time.

     Below are the steps involved to understand, clean and prepare your data for building your predictive model:

     1. Variable Identification
     2. Univariate Analysis
     3. Bi-variate Analysis
     4. Missing values treatment
     5. Outlier treatment
     6. Variable transformation
     7. Variable creation

     Finally, we will need to iterate over steps 4 – 7 multiple times before we come up with our refined model.

     Let’s now study each stage in  detail:-

      

     ### Variable Identification

     First, identify **Predictor** (Input) and **Target** (output) variables. Next, identify the data type and category of the variables.

     Let’s understand this step more clearly by taking an example.

     Example:- Suppose, we want to predict, whether the students will play cricket or not (refer below data set). Here you need to identify predictor variables, target variable, data type of variables and category of variables.[![商业分析、数据探索](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_11.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_11.png)Below, the variables have been defined in different category:

     [![商业分析、数据探索](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_2.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_2.png)

      

     ### Univariate Analysis

     At this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. Let’s look at these methods and statistical measures for categorical and continuous variables individually:

     **Continuous Variables:-** In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics visualization methods as shown below:

     [![数据探索、商业分析](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_31.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_31.png)**Note:** Univariate analysis is also used to highlight missing and outlier values. In the upcoming part of this series, we will look at methods to handle missing and outlier values. To know more about these methods, you can refer course [descriptive statistics from Udacity](https://www.udacity.com/course/ud827).

     **Categorical Variables:-** For categorical variables, we’ll use frequency table to understand distribution of each category. We can also read as percentage of values under each category. It can be be measured using two metrics, **Count** and **Count%** against each category. Bar chart can be used as visualization.

      

     ### Bi-variate Analysis

     双变量分析找出两个变量之间的关系。在这里，我们在预定义的显着性水平上寻找变量之间的关联和分离。我们可以对分类变量和连续变量的任意组合进行双变量分析。组合可以是：分类和分类、分类和连续以及连续和连续。在分析过程中使用不同的方法来处理这些组合。

     让我们详细了解可能的组合：

     **连续和连续：**在两个连续变量之间进行双变量分析时，我们应该查看散点图。这是找出两个变量之间关系的好方法。散点图的模式表示变量之间的关系。该关系可以是线性的或非线性的。

     [![数据探索、商业分析](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_4.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_4.png)散点图显示了两个变量之间的关系，但不表示它们之间的关系强度。为了找到关系的强度，我们使用相关性。相关性在 -1 和 +1 之间变化。

     - -1：完美的负线性相关
     - +1：完美的正线性相关和 [
       ](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_4.png)
     - 0：无相关性

     可以使用以下公式导出相关性：

     **相关性 = 协方差(X,Y) / SQRT( Var(X)\* Var(Y))**

     各种工具具有识别变量之间相关性的功能或功能。在 Excel 中，函数 CORREL() 用于返回两个变量之间的相关性，而 SAS 使用过程 PROC CORR 来识别相关性。这些函数返回皮尔逊相关值以识别两个变量之间的关系：

     [![相关性、协方差、方差、数据探索、业务分析](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_51.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_51.png)

     在上面的例子中，我们在两个变量 X 和 Y 之间有良好的正相关关系（0.65）。

      

     **Categorical & Categorical：** 要找到两个分类变量之间的关系，我们可以使用以下方法：

     - **双向表：**我们可以通过创建 count 和 count% 的双向表来开始分析关系。行代表一个变量的类别，列代表另一个变量的类别。我们在行和列类别的每个组合中显示可用观察的计数或计数百分比。
     - **Stacked Column Chart：**这种方法更像是一种双向表的可视化形式。

     [![数据探索、商业分析、堆积柱状图、双向表](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_6-1024x183.gif)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_6.gif)

     - **卡方检验：** 该检验用于推导出变量之间关系的统计显着性。此外，它还测试样本中的证据是否足够强大，可以将这种关系也推广到更大的群体。卡方基于双向表中一个或多个类别的预期频率和观察频率之间的差异。它返回具有自由度的计算卡方分布的概率。

     概率为 0：表示两个分类变量都是相关的

     概率为 1：表明两个变量是独立的。

     概率小于 0.05：表示变量之间的关系在 95% 的置信度下显着。两个分类变量的独立性检验的卡方检验统计量通过以下方式找到：

     [![数据探索、卡方、商业分析](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_7.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_7.png)其中*O*表示观察到的频率。*E*是原假设下的预期频率，计算公式为： 从前面的双向表中，小尺寸产品类别 1 的预期计数为 0.22。它是通过将大小 (9) 的行总数乘以产品类别 (2) 的列总数然后除以样本大小 (81) 得出的。这是对每个单元格进行的程序。用于分析关系效力的统计措施是：
     [![数据探索、卡方、商业分析](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_8.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_exploration_8.png)

     - 用于名义分类变量的 Cramer's V
     - 序数分类变量的 Mantel-Haenszed 卡方。

     不同的数据科学语言和工具具有执行卡方检验的特定方法。在 SAS 中，我们可以使用**Chisq** 作为**Proc freq**的选项来执行此测试。

      

     **分类和连续：**在探索分类变量和连续变量之间的关系时，我们可以为分类变量的每个级别绘制箱线图。如果级别数较少，则不会显示统计显着性。要查看统计显着性，我们可以执行 Z 检验、T 检验或方差分析。

     - **Z 检验/ T 检验：-**任一检验评估两组的平均值是否在统计上彼此不同。[![测试公式](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/ztestformula1.jpg)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/ztestformula1.jpg)如果 Z 的概率很小，则两个平均值的差异更显着。T 检验与 Z 检验非常相似，但在两个类别的观察数小于 30 时使用。
       [![数据探索、商业分析](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/ttest.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/ttest.png)
     - **方差分析：-**它评估两组以上的平均值是否具有统计学差异。

     **示例：**假设我们要测试五种不同练习的效果。为此，我们招募了 20 名男性，并将一种类型的运动分配给 4 名男性（5 组）。几周后记录他们的体重。我们需要找出这些练习对他们的影响是否有显着差异。这可以通过比较 5 组每组 4 人的体重来完成。

     至此，我们已经了解了数据探索、变量识别、单变量和双变量分析的前三个阶段。我们还研究了各种统计和视觉方法来确定变量之间的关系。 

     

     现在，我们将看看缺失值处理的方法。更重要的是，我们还将研究为什么数据中会出现缺失值以及为什么需要处理它们。

      

     ## 2. 缺失值处理

      

     ### 为什么需要进行缺失值处理？

     训练数据集中的缺失数据会降低模型的功效/拟合，或者可能导致模型有偏差，因为我们没有正确分析行为和与其他变量的关系。它可能导致错误的预测或分类。

     [![数据探索，缺失值](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_Exploration_2_11.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_Exploration_2_11.png)

     注意上图中的缺失值：在左边的场景中，我们没有处理缺失值。从这个数据集推断，男性打板球的机会高于女性。另一方面，如果您查看第二个表，其中显示了缺失值（基于性别）处理后的数据，我们可以看到，与男性相比，女性打板球的机会更高。

      

     ### 为什么我的数据有缺失值？

     我们研究了处理数据集中缺失值的重要性。现在，让我们找出出现这些缺失值的原因。它们可能发生在两个阶段：

     1. **数据提取**：提取过程可能存在问题。在这种情况下，我们应该与数据监护人一起仔细检查正确的数据。一些散列程序也可用于确保数据提取正确。数据提取阶段的错误通常很容易发现，也很容易纠正。

     2. 数据收集

        ：这些错误发生在数据收集时，更难纠正。它们可以分为四种类型：

        - **完全随机丢失：** 这是所有观测值丢失变量的概率相同的情况。例如：数据收集过程的受访者决定在掷一枚公平的硬币后宣布他们的收入。如果出现人头，受访者会申报他/她的收入，反之亦然。这里每个观察值都有相等的缺失值机会。
        - **随机缺失：**这是随机缺失变量并且缺失率因其他输入变量的不同值/水平而变化的情况。例如：我们正在收集年龄数据，与男性相比，女性的缺失值更高。
        - **缺失取决于未观察到的预测变量：** 这是缺失值不是随机的并且与未观察到的输入变量相关的情况。例如：在医学研究中，如果特定诊断引起不适，则退出研究的可能性更高。这个缺失值不是随机的，除非我们将“不适”作为所有患者的输入变量。
        - **缺失取决于缺失值本身：**这是缺失值概率与缺失值本身直接相关的情况。例如： 收入较高或较低的人很可能对他们的收入没有反应。

      

     ### 处理缺失值的方法有哪些？

     1. 删除： 

        分为两种：List Wise Deletion 和 Pair Wise Deletion。

        - 在列表明智的删除中，我们删除缺少任何变量的观察。简单性是这种方法的主要优点之一，但这种方法降低了模型的功效，因为它减少了样本量。
        - 在成对删除中，我们对存在感兴趣变量的所有情况进行分析。这种方法的优点是，它保留了尽可能多的案例可用于分析。这种方法的缺点之一是它对不同的变量使用不同的样本量。
          [
          ](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_Exploration_2_2.png)[![数据探索、缺失值、删除方法](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_Exploration_2_2.png)](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_Exploration_2_2.png)
        - 当缺失数据的性质是“**完全**随机缺失”时使用删除方法，否则非随机缺失值会使模型输出产生偏差。

2. 均值/众数**/中值插补：** 插补是一种用估计值填充缺失值的方法。目标是利用可以在数据集的有效值中识别的已知关系来帮助估计缺失值。均值/众数/中值插补是最常用的方法之一。它包括用该变量的所有已知值的平均值或中位数（定量属性）或众数（定性属性）替换给定属性的缺失数据。它可以有两种类型：-

- **广义插补：**在这种情况下，我们计算该变量所有非缺失值的平均值或中位数，然后用平均值或中位数替换缺失值。与上表一样，变量“ **Manpower”**缺失，因此我们取“ **Manpower”**  （**28.33**）所有非缺失值的平均值，然后用它替换缺失值。
- **类似情况插补：**在这种情况下，我们分别计算非缺失值的性别“**男性”** （29.75）和“**女性**”（25）的平均值，然后根据性别替换缺失值。对于“**男性**”，我们将人力的缺失值替换为 29.75，将“**女性**”的缺失值替换为25。

3. 预测模型：我们创建一个预测模型来评估将替代缺失数据的值。我们将数据集分为两组：一组没有变量缺失值，另一组有缺失值。具有确实值的变量被视为目标变量。接下来，创建一个模型来根据训练数据集的其他属性预测目标变量并填充测试数据集的缺失值。我们可以使用回归，方差分析，逻辑回归和各种建模技术来执行此操作。这种方法有两个缺点：
   - 模型估计值通常比真实值表现得更好
   - 如果数据集中的属性和具有缺失值的属性没有关系，那么模型将无法精确估计缺失值。
4. KNN插补：在这种插补方法中，使用给定数量的与缺失值的属性最相似的属性来插补属性的缺失值。使用距离函数确定两个属性的相似性。众所周知，它具有某些优点和缺点。

- 好处：
  - k-最近邻可以预测定性和定量属性
  - 不需要为缺失数据的每个属性创建预测模型
  - 可以轻松处理具有多个缺失值的属性
  - 考虑数据的相关结构
- 坏处：
  - KNN算法在分析大型数据库时非常耗时。它搜索所有数据集以寻找最相似的实例。
  - k 值的选择非常关键。较高的 k 值将包括与我们需要的显着不同的属性，而较低的 k 值意味着缺少重要属性。

## ３.　异常值检测与处理技术

**什么是异常值？**

异常值是分析师和数据科学家常用的术语，因为它需要密切关注，否则可能会导致非常错误的估计。简单地说，离群值是一个看起来很远并且与样本中的整体模式不同的观察值。

通常，我们在构建模型时倾向于忽略异常值。这是一种令人沮丧的做法。异常值往往会使您的数据倾斜并降低准确性。

**异常值的类型有哪些？**

离群值可以有两种类型：**单变量**和 **多变量**。上面，我们已经讨论了单变量异常值的例子。当我们查看单个变量的分布时，可以找到这些异常值。多变量异常值是 n 维空间中的异常值。为了找到它们，您必须查看多维分布。

让我们通过一个例子来理解这一点。假设我们正在了解身高和体重之间的关系。下面，我们有身高、体重的单变量和双变量分布。看一下箱线图。我们没有任何异常值（高于和低于 1.5*IQR，最常用的方法）。现在看散点图。在这里，在特定的体重和身高部分，我们有两个低于平均值的值，一个高于平均值。

![](E:\moon\大二\计算机视觉+深度学习\first_month\Outlier_21-850x216.webp)

异常值可以极大地改变数据分析和统计建模的结果。数据集中的异常值有许多不利影响：

- 它增加了误差方差并降低了统计检验的功效
- 如果异常值是非随机分布的，它们会降低正态性
- 他们可能会偏向或影响可能具有实质性利益的估计
- 它们还会影响回归的基本假设、方差分析和其他统计模型假设。

**如何检查异常值？**

最常规的方法是可视化，例如：**Box-plot**、**Histogram**、**Scatter Plot**

- 任何超出 -1.5 x IQR 到 1.5 x IQR 范围的值
- 使用封顶方法。任何超出第 5 个和第 95 个百分位数范围的值都可以被视为异常值
- 离均值三个或三个以上标准差的数据点被认为是异常值
- 离群点检测只是检查有影响数据点的数据的一个特例，它也取决于业务理解
- 双变量和多变量异常值通常使用影响指数或杠杆指数或距离来衡量。马哈拉诺比斯距离和库克*D*等流行指标经常用于检测异常值。
- 在SAS中，我们可以使用PROC Univariate，PROC SGPLOT。为了识别异常值和有影响的观察结果，我们还会查看统计指标，如 STUDENT、COOKD、RSTUDENT 等。

**如何去除异常值？**

**删除观察** 值：如果由于数据输入错误、数据处理错误或异常值观察数量非常少，我们将删除异常值。我们还可以在两端使用修剪来去除异常值。

**转换和分箱值：** 转换变量还可以消除异常值。一个值的自然对数减少了由极值引起的变化。分箱也是变量变换的一种形式。由于变量的分箱，决策树算法允许很好地处理异常值。我们还可以使用为不同的观察分配权重的过程。

![](E:\moon\大二\计算机视觉+深度学习\first_month\Transformation_1-850x251.webp)

插补**：** 与缺失值插补一样 ，我们也可以插补异常值。我们可以使用均值、中值、模式插补方法。在插补值之前，我们应该分析它是自然异常值还是人工值。如果它是人为的，我们可以使用估算值。我们还可以使用统计模型来预测异常值观察值，然后我们可以用预测值对其进行估算。

**单独处理：** 如果有大量异常值，我们应该在统计模型中单独处理它们。一种方法是将两个组视为两个不同的组，并为这两个组构建单独的模型，然后组合输出。

## 4. 特征工程：

### 什么是特征工程？

特征工程是从现有数据中提取更多信息的科学（和艺术）。您没有在此处添加任何新数据，但实际上是在使您已有的数据更有用。

特征工程本身可以分为两步：

- 变量变换。
- 变量/特征创建。

### 什么是变量变换？

在数据建模中，转换是指用函数替换变量。用平方/立方根或对数x替换变量x是一种变换。换句话说，变量变换是改变一个变量与其他变量的分布或关系的过程。

### 我们什么时候应该使用变量转换？

以下是需要进行变量转换的情况：

- 当我们想要**改变**一个变量**的尺度**或标准化一个变量的值以便更好地理解时。如果您有不同尺度的数据，则必须进行此转换，但此转换不会改变变量分布的形状

- 当我们可以**将复杂的非线性关系转化为线性关系时**。与非线性或曲线关系相比，变量之间存在线性关系更容易理解。变换帮助我们将非线性关系转换为线性关系。散点图可用于找出两个连续变量之间的关系。这些转换也改进了预测。对数转换是这些情况下常用的转换技术之一。

![](E:\moon\大二\计算机视觉+深度学习\first_month\Relation.png)

- **对称分布优于偏态分布，**因为它更容易解释和生成推论。一些建模技术需要变量的正态分布。因此，每当我们有偏斜分布时，我们都可以使用减少偏斜的变换。对于右偏态分布，我们取变量的平方/立方根或对数，对于左偏态，我们取变量的平方/立方或指数。

> 5种简单的操作可从数据中提取更多的信息
>
> https://www.analyticsvidhya.com/blog/2013/11/simple-manipulations-extract-data/

